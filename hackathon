import os
import pypandoc
from bs4 import BeautifulSoup
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

nltk.download('punkt')

# --- Settings ---
INPUT_FOLDER = "documents"
SUMMARY_RATIO = 0.3  # top 30% sentences for summary

# --- Function to extract text from DOCX ---
def extract_docx_text(docx_path):
    html = pypandoc.convert_file(docx_path, 'html')
    soup = BeautifulSoup(html, 'html.parser')
    
    # Paragraphs
    paragraphs = soup.find_all('p')
    para_text = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]
    
    # Tables
    tables = soup.find_all('table')
    table_rows = []
    for table in tables:
        for tr in table.find_all('tr'):
            cells = [td.get_text().strip() for td in tr.find_all(['td','th']) if td.get_text().strip()]
            if cells:
                row_text = ' | '.join(cells)
                table_rows.append(row_text)
    
    full_text = '\n'.join(para_text + table_rows)
    return full_text

# --- Function to summarize text ---
def summarize_text(text, ratio=SUMMARY_RATIO):
    sentences = nltk.sent_tokenize(text)
    if not sentences:
        return ""
    
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(sentences)
    scores = X.sum(axis=1).A1
    
    top_n = max(1, int(len(sentences) * ratio))
    top_indices = np.argsort(scores)[-top_n:]
    top_indices.sort()
    
    summary = ' '.join([sentences[i] for i in top_indices])
    return summary

# --- Get first DOCX in folder ---
docx_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith('.docx')]
if not docx_files:
    raise FileNotFoundError("No DOCX files found in the folder.")

first_docx_path = os.path.join(INPUT_FOLDER, docx_files[0])

# --- Extract text and summarize ---
text = extract_docx_text(first_docx_path)
summary = summarize_text(text)

print(f"Summary of '{docx_files[0]}':\n")
print(summary)
